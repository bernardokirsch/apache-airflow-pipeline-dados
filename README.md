# Primeiros Passos com Apache Airflow: Construindo um Pipeline de Dados do Zero

RepositÃ³rio referente aos cÃ³digos e documentaÃ§Ãµes do **Minicurso â€œPrimeiros Passos com Apache Airflow: Construindo um Pipeline de Dados do Zeroâ€**.

---

## ğŸ“˜ VisÃ£o Geral

O objetivo deste projeto Ã© **demonstrar na prÃ¡tica os fundamentos do Apache Airflow**, ferramenta lÃ­der de orquestraÃ§Ã£o de pipelines de dados.  
A proposta Ã© construir um pipeline ETL **completo e automatizado**, entendendo o funcionamento real das engrenagens que sustentam projetos modernos de **Engenharia de Dados**.

### ğŸ¯ Objetivos do Minicurso

1. Subir o **Apache Airflow** em containers via **Docker Compose**.  
2. Compreender os conceitos de **DAGs, Tasks e Operators**.  
3. Criar um **pipeline ETL completo**, capaz de:
   - **Extrair** informaÃ§Ãµes de uma **API pÃºblica**.  
   - **Transformar** os dados utilizando **Python (Pandas)**.  
   - **Carregar** o resultado em um **arquivo CSV** organizado em um diretÃ³rio de *datalake*.  

---

## ğŸ§  Arquitetura do Projeto

```shell
ğŸ“ apache-airflow-pipeline-dados
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ dag_cnpj_query_etl_pipeline.py 
â”‚   â”‚   â”œâ”€â”€ dag_first_etl_pipeline.py 
â”‚   â”‚   â”œâ”€â”€ dag_ibge_estados_etl_pipeline.py
â”‚   â”‚   â””â”€â”€ ... 
â”‚   â”œâ”€â”€ logs/ 
â”‚   â””â”€â”€ plugins/                                     
â”œâ”€â”€ datalake/ 
â”‚   â”œâ”€â”€ bronze/ 
â”‚   â”œâ”€â”€ silver/ 
â”‚   â””â”€â”€ gold/ 
â”œâ”€â”€ .env 
â”œâ”€â”€ docker-compose.yaml 
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt 
â”œâ”€â”€ setup.ps1  # Script de inicializaÃ§Ã£o (Windows)
â””â”€â”€ setup.sh   # Script de inicializaÃ§Ã£o (Linux/WSL)
```

### ğŸ“Š Estrutura do Datalake

- **Bronze:** dados brutos extraÃ­dos da API.  
- **Silver:** dados transformados e limpos (pÃ³s-Pandas).  
- **Gold:** relatÃ³rios e datasets prontos para consumo analÃ­tico.

---

## âš™ï¸ Tecnologias Utilizadas

| Categoria | Ferramenta |
|------------|-------------|
| OrquestraÃ§Ã£o | **Apache Airflow 2.11** |
| Linguagem | **Python 3.11** |
| ContainerizaÃ§Ã£o | **Docker + Docker Compose** |
| TransformaÃ§Ã£o de Dados | **Pandas** |
| RequisiÃ§Ãµes HTTP | **Requests** |
| Ambiente | **Windows (PowerShell) / Linux (WSL2)** |

---

## ğŸ§© PrÃ©-requisitos

Certifique-se de ter os seguintes itens instalados:

- [Docker Desktop](https://www.docker.com/products/docker-desktop/)
- [Python 3.11+](https://www.python.org/downloads/)
- [Git](https://git-scm.com/downloads)
- **Windows PowerShell** ou **Linux Bash/WSL**

---

## âš¡ Setup AutomÃ¡tico

Este projeto jÃ¡ possui scripts automatizados para inicializaÃ§Ã£o do ambiente.

### ğŸªŸ No Windows (PowerShell)

```powershell
# Clonar o repositÃ³rio
git clone https://github.com/bernardokirsch/apache-airflow-pipeline-dados.git
cd apache-airflow-pipeline-dados

# Executar o setup automatizado
.\setup.ps1
```
